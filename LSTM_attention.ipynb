{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  # TO DO: Añadir dropout \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
    "        output = self.fc_out(output)\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "class Bilinear_Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super(Bilinear_Attention, self).__init__()\n",
    "        self.W = nn.Linear(decoder_dim, encoder_dim, bias = False)\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        decoder_attn = self.W(decoder_hidden)\n",
    "        print(decoder_attn)\n",
    "        score = torch.matmul(decoder_attn, encoder_out.transpose(1,2))\n",
    "        print(score)\n",
    "        att_weights = torch.softmax(score, dim = 0)\n",
    "        print(att_weights)\n",
    "        att = att_weights.matmul(encoder_out)\n",
    "        return att\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder                           \n",
    "        self.attention = attention\n",
    "        self.fc = nn.Linear(hiden_dim + attn_dim, hidden_dim)\n",
    "        self.es_embeddings = torchtext.vocab.FastText(language='es')\n",
    "        self.M = self.es_embeddings.vectors\n",
    "        self.M = torch.cat((self.M, torch.zeros((4, self.M.shape[1]))), 0)\n",
    "        \n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        target_len = target.shape[1]\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        # Tensor para almacenar las salidas del decoder\n",
    "        outputs = torch.zeros(batch_size, target_len, 985671)\n",
    "        \n",
    "        # Primero, la fuente es procesada por el encoder\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(source)\n",
    "\n",
    "        # La primera entrada al decoder es el vector <sos>\n",
    "        x = target[:, 0, :]\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            attention = self.attention(encoder_outputs, hidden)\n",
    "            new_hidden = torch.cat(hidden,attention, dim = 0)\n",
    "            new_hidden = self.fc(new_hidden)\n",
    "            hidden = torch.tanh(new_hidden)\n",
    "            output, (hidden, cell) = self.decoder(x.unsqueeze(1), hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if teacher_force:\n",
    "                x = target[:, t, :]\n",
    "            else:\n",
    "                x = torch.matmul(output.squeeze(1), self.M)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([-0.1313, -0.7526,  0.3534, -1.1248, -0.0745],\n",
      "       grad_fn=<SqueezeBackward4>)\n",
      "tensor([[-1.7297, -1.7297, -1.7297]], grad_fn=<UnsafeViewBackward0>)\n",
      "tensor([[1., 1., 1.]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[[3., 3., 3., 3., 3.]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# SANITY CHECk\n",
    "encoder_out = torch.ones((1,3,5))\n",
    "decoder_hid = torch.ones(5)\n",
    "print(encoder_out)\n",
    "print(decoder_hid)\n",
    "\n",
    "attention_test  = Bilinear_Attention(5,5)\n",
    "\n",
    "\n",
    "print(attention_test.forward(encoder_out, decoder_hid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/europarl/europarl-v7.es-en.en'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adria\\Documents\\GitHub\\SEQ2SEQ_Attention\\LSTM_attention.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m archivo_espanol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/europarl/europarl-v7.es-en.es\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Leer el conjunto de datos\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, (ingles, espanol) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(read_translation(archivo_ingles, archivo_espanol)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mInglés:\u001b[39m\u001b[39m'\u001b[39m, ingles)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mEspañol:\u001b[39m\u001b[39m'\u001b[39m, espanol)\n",
      "\u001b[1;32mc:\\Users\\adria\\Documents\\GitHub\\SEQ2SEQ_Attention\\LSTM_attention.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_translation\u001b[39m(archivo_ingles, archivo_espanol):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(archivo_ingles, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f_ingles, \u001b[39mopen\u001b[39m(archivo_espanol, \u001b[39m'\u001b[39m\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f_espanol:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39mfor\u001b[39;00m oracion_ingles, oracion_espanol \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(f_ingles, f_espanol):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             \u001b[39myield\u001b[39;00m oracion_ingles\u001b[39m.\u001b[39mstrip(), oracion_espanol\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/europarl/europarl-v7.es-en.en'"
     ]
    }
   ],
   "source": [
    "#Dataset Europarl\n",
    "\n",
    "def read_translation(archivo_ingles, archivo_espanol):\n",
    "    with open(archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "        for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "            yield oracion_ingles.strip(), oracion_espanol.strip()\n",
    "\n",
    "\n",
    "archivo_ingles = 'data/europarl/europarl-v7.es-en.en'\n",
    "archivo_espanol = 'data/europarl/europarl-v7.es-en.es'\n",
    "\n",
    "# Leer el conjunto de datos\n",
    "for i, (ingles, espanol) in enumerate(read_translation(archivo_ingles, archivo_espanol)):\n",
    "    print('Inglés:', ingles)\n",
    "    print('Español:', espanol)\n",
    "    print('---')\n",
    "    if i == 15:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

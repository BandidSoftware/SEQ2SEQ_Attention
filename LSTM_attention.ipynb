{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torchtext\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translation(Dataset):\n",
    "    def __init__(self, source_file, target_file):\n",
    "        self.ingles = []\n",
    "        self.espanol = []\n",
    "        self.tokenizer_es = get_tokenizer(\"spacy\", language=\"es_core_news_md\")\n",
    "        self.tokenizer_en = get_tokenizer(\"spacy\", language=\"en_core_web_md\")\n",
    "        self.vocab_es = torchtext.vocab.FastText(language='es', unk_init=torch.Tensor.normal_)  # <-- Mirar esto para ver si añadir el token <unk> al vocabulario\n",
    "        self.vocab_en = torchtext.vocab.FastText(language='en', unk_init=torch.Tensor.normal_)\n",
    "\n",
    "        self.vocab_en = self.add_sos_eos_unk_pad(self.vocab_en)\n",
    "        self.vocab_es = self.add_sos_eos_unk_pad(self.vocab_es)\n",
    "\n",
    "        self.archivo_ingles = source_file\n",
    "        self.archivo_espanol = target_file\n",
    "\n",
    "        # Leer el conjunto de datos\n",
    "        for ingles, espanol in self.read_translation():\n",
    "            self.ingles.append(ingles)\n",
    "            self.espanol.append(espanol)\n",
    "\n",
    "\n",
    "    def add_sos_eos_unk_pad(self, vocabulary):\n",
    "        words = vocabulary.itos\n",
    "        vocab = vocabulary.stoi\n",
    "        embedding_matrix = vocabulary.vectors\n",
    "\n",
    "        # Tokens especiales\n",
    "        sos_token = '<sos>'\n",
    "        eos_token = '<eos>'\n",
    "        pad_token = '<pad>'\n",
    "        unk_token = '<unk>'\n",
    "\n",
    "        # Inicializamos los vectores para los tokens especiales, por ejemplo, con ceros\n",
    "        sos_vector = torch.full((1, embedding_matrix.shape[1]), 1.)\n",
    "        eos_vector = torch.full((1, embedding_matrix.shape[1]), 2.)\n",
    "        pad_vector = torch.zeros((1, embedding_matrix.shape[1]))\n",
    "        unk_vector = torch.full((1, embedding_matrix.shape[1]), 3.)\n",
    "\n",
    "        # Añade los vectores al final de la matriz de embeddings\n",
    "        embedding_matrix = torch.cat((embedding_matrix, sos_vector, eos_vector, unk_vector, pad_vector), 0)\n",
    "\n",
    "        # Añade los tokens especiales al vocabulario\n",
    "        vocab[sos_token] = len(vocab)\n",
    "        vocab[eos_token] = len(vocab)\n",
    "        vocab[pad_token] = len(vocab)\n",
    "        vocab[unk_token] = len(vocab)\n",
    "\n",
    "        words.append(sos_token)\n",
    "        words.append(eos_token)\n",
    "        words.append(pad_token)\n",
    "        words.append(unk_token)\n",
    "\n",
    "        vocabulary.itos = words\n",
    "        vocabulary.stoi = vocab\n",
    "        vocabulary.vectors = embedding_matrix\n",
    "\n",
    "        default_stoi = defaultdict(lambda : len(vocabulary)-1, vocabulary.stoi)\n",
    "        vocabulary.stoi = default_stoi\n",
    "    \n",
    "        return vocabulary\n",
    "        \n",
    "\n",
    "    def read_translation(self):\n",
    "        with open(self.archivo_ingles, 'r', encoding='utf-8') as f_ingles, open(self.archivo_espanol, 'r', encoding='utf-8') as f_espanol:\n",
    "            for oracion_ingles, oracion_espanol in zip(f_ingles, f_espanol):\n",
    "                yield oracion_ingles.strip().lower(), oracion_espanol.strip().lower()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ingles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.ingles[idx], self.espanol[idx]\n",
    "        tokens_ingles = self.tokenizer_en(item[0])\n",
    "        tokens_espanol = self.tokenizer_es(item[1])\n",
    "\n",
    "        tokens_ingles = tokens_ingles + ['<eos>']\n",
    "        tokens_espanol = ['<sos>'] + tokens_espanol + ['<eos>']\n",
    "\n",
    "        if not tokens_ingles or not tokens_espanol:\n",
    "            return torch.zeros(1, 300), torch.zeros(1, 300)\n",
    "            # raise RuntimeError(\"Una de las muestras está vacía.\")\n",
    "    \n",
    "        tensor_ingles = self.vocab_en.get_vecs_by_tokens(tokens_ingles)\n",
    "        tensor_espanol = self.vocab_es.get_vecs_by_tokens(tokens_espanol)\n",
    "\n",
    "        indices_ingles = [self.vocab_en.stoi[token] for token in tokens_ingles] + [self.vocab_en.stoi['<pad>']]\n",
    "        indices_espanol = [self.vocab_es.stoi[token] for token in tokens_espanol] + [self.vocab_es.stoi['<pad>']]\n",
    "\n",
    "        return tensor_ingles, tensor_espanol, indices_ingles, indices_espanol\n",
    "        \n",
    "            \n",
    "        \n",
    "def collate_fn(batch):\n",
    "    ingles_batch, espanol_batch, ingles_seqs, espanol_seqs = zip(*batch)\n",
    "    ingles_batch = pad_sequence(ingles_batch, batch_first=True, padding_value=0)\n",
    "    espanol_batch = pad_sequence(espanol_batch, batch_first=True, padding_value=0)\n",
    "\n",
    "    # Calcular la longitud máxima de la lista de listas de índices\n",
    "    pad = espanol_seqs[0][-1]  # token <pad>\n",
    "    max_len = max([len(l) for l in espanol_seqs])\n",
    "    for seq in espanol_seqs:\n",
    "        seq += [pad]*(max_len-len(seq))\n",
    "        \n",
    "    return ingles_batch.to(device), espanol_batch.to(device), ingles_seqs, espanol_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please install SpaCy. See the docs at https://spacy.io for more information.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m archivo_ingles \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/es-en/europarl-v7.es-en.en\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m archivo_espanol \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdata/es-en/europarl-v7.es-en.es\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m translation \u001b[39m=\u001b[39m Translation(archivo_ingles, archivo_espanol)\n",
      "\u001b[1;32m/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb Cell 3\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mingles \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mespanol \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer_es \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m\"\u001b[39;49m\u001b[39mspacy\u001b[39;49m\u001b[39m\"\u001b[39;49m, language\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mes_core_news_md\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer_en \u001b[39m=\u001b[39m get_tokenizer(\u001b[39m\"\u001b[39m\u001b[39mspacy\u001b[39m\u001b[39m\"\u001b[39m, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39men_core_web_md\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/adrian/clases/PLN/SEQ2SEQ_Attention/LSTM_attention.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab_es \u001b[39m=\u001b[39m torchtext\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mFastText(language\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mes\u001b[39m\u001b[39m'\u001b[39m, unk_init\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mTensor\u001b[39m.\u001b[39mnormal_)  \u001b[39m# <-- Mirar esto para ver si añadir el token <unk> al vocabulario\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PLN/lib/python3.11/site-packages/torchtext/data/utils.py:91\u001b[0m, in \u001b[0;36mget_tokenizer\u001b[0;34m(tokenizer, language)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mspacy\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     90\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mspacy\u001b[39;00m\n\u001b[1;32m     93\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     94\u001b[0m             spacy \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(language)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# archivo_ingles = 'data/europarl/europarl-v7.es-en.en'\n",
    "# archivo_espanol = 'data/europarl/europarl-v7.es-en.es'\n",
    "\n",
    "archivo_ingles = 'data/es-en/europarl-v7.es-en.en'\n",
    "archivo_espanol = 'data/es-en/europarl-v7.es-en.es'\n",
    "\n",
    "translation = Translation(archivo_ingles, archivo_espanol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super().__init__() \n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, (hidden, cell) = self.rnn(x)\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  # TO DO: Añadir dropout \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        output, (hidden, cell) = self.rnn(x, (hidden, cell))\n",
    "        output = self.fc_out(output)\n",
    "        return output, (hidden, cell)\n",
    "\n",
    "class Bilinear_Attention(nn.Module):\n",
    "    def __init__(self, encoder_dim, decoder_dim):\n",
    "        super(Bilinear_Attention, self).__init__()\n",
    "        self.W = nn.Linear(decoder_dim, encoder_dim, bias = False)\n",
    "        self.fc = nn.Linear(decoder_dim + encoder_dim, decoder_dim)\n",
    "        \n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        decoder_attn = self.W(decoder_hidden)\n",
    "        score = torch.matmul(decoder_attn, encoder_out.transpose(1,2))\n",
    "        att_weights = torch.softmax(score, dim = 1)\n",
    "        att = torch.sum(torch.matmul(att_weights,encoder_out), dim = 1)\n",
    "        \n",
    "        new_hidden = torch.cat((decoder_hidden, att.unsqueeze(0)) , dim = 2)\n",
    "        new_hidden = self.fc(new_hidden)\n",
    "        new_hidden = torch.tanh(new_hidden)\n",
    "\n",
    "        return new_hidden\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, attention):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder                           \n",
    "        self.attention = attention\n",
    "        self.es_embeddings = torchtext.vocab.FastText(language='es')\n",
    "        self.M = self.es_embeddings.vectors\n",
    "        self.M = torch.cat((self.M, torch.zeros((4, self.M.shape[1]))), 0)\n",
    "        \n",
    "\n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        target_len = target.shape[1]\n",
    "        batch_size = target.shape[0]\n",
    "\n",
    "        # Tensor para almacenar las salidas del decoder\n",
    "        outputs = torch.zeros(batch_size, target_len, 985671)\n",
    "        \n",
    "        # Primero, la fuente es procesada por el encoder\n",
    "        encoder_outputs, (hidden, cell) = self.encoder(source)\n",
    "        print(encoder_outputs.shape)\n",
    "        print(hidden.shape)\n",
    "        # La primera entrada al decoder es el vector <sos>\n",
    "        x = target[:, 0, :]\n",
    "        output, (hidden, cell) = self.decoder(x.unsqueeze(1), hidden, cell)\n",
    "        \n",
    "        for t in range(1, target_len):            \n",
    "            hidden = self.attention(encoder_outputs, hidden)\n",
    "            output, (hidden, cell) = self.decoder(x.unsqueeze(1), hidden, cell)\n",
    "            outputs[:, t, :] = output.squeeze(1)\n",
    "            \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            if teacher_force:\n",
    "                x = target[:, t, :]\n",
    "            else:\n",
    "                x = torch.matmul(output.squeeze(1), self.M)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamiento\n",
    "\n",
    "# Parámetros\n",
    "input_dim = 300\n",
    "output_dim = translation.vocab_es.vectors.shape[0]\n",
    "hidden_dim = 512\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30\n",
    "batch_size = 8\n",
    "num_workers = 0\n",
    "shuffle = True\n",
    "\n",
    "# Inicializa el modelo, el optimizador y la función de pérdida\n",
    "encoder = Encoder(input_dim, hidden_dim, num_layers)\n",
    "decoder = Decoder(input_dim, hidden_dim, output_dim, num_layers)\n",
    "attention = Bilinear_Attention(hidden_dim, hidden_dim)\n",
    "model = Seq2Seq(encoder, decoder, attention).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(translation, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 76, 512])\n",
      "torch.Size([2, 8, 512])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\adria\\Documents\\GitHub\\SEQ2SEQ_Attention\\LSTM_attention.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, (src, tgt, src_indices, tgt_indices) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     output \u001b[39m=\u001b[39m model(src, tgt)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     tgt_indices \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(tgt_indices, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\adria\\Documents\\GitHub\\SEQ2SEQ_Attention\\LSTM_attention.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m output, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hidden, cell)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, target_len):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     hidden \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(encoder_outputs, hidden)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     output, (hidden, cell) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m), hidden, cell)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     outputs[:, t, :] \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\adria\\miniconda3\\envs\\cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\adria\\Documents\\GitHub\\SEQ2SEQ_Attention\\LSTM_attention.ipynb Cell 7\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, encoder_out, decoder_hidden):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     decoder_attn \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW(decoder_hidden)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(decoder_attn, encoder_out\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     att_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(score, dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/adria/Documents/GitHub/SEQ2SEQ_Attention/LSTM_attention.ipynb#X10sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     att \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(att_weights\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m),encoder_out)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (8) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Definir el directorio donde se guardarán los modelos\n",
    "save_dir = 'trained_models/'\n",
    "\n",
    "# Bucle de entrenamiento\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (src, tgt, src_indices, tgt_indices) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        tgt_indices = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "        loss = 0\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            loss += criterion(output[:, t, :], tgt_indices[:, t])\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 5 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(dataloader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {total_loss / len(dataloader):.4f}')\n",
    "\n",
    "    # Guardar el modelo cada 5 épocas\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_path = f'modelo.pth'\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f'Modelo guardado en {save_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with input sentences\n",
    "model.eval()\n",
    "\n",
    "sentence = \"tiger\"\n",
    "\n",
    "# Convertir a vectores\n",
    "tokens = translation.tokenizer_en(sentence)\n",
    "tokens = tokens + ['<eos>']\n",
    "text_tensor = translation.vocab_en.get_vecs_by_tokens(tokens)\n",
    "text_tensor = text_tensor.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_outputs, (hidden, cell) = model.encoder(text_tensor)\n",
    "\n",
    "outputs = []\n",
    "\n",
    "input_token = torch.tensor(translation.vocab_es.stoi['<sos>']).unsqueeze(0)\n",
    "input_token = translation.vocab_es.vectors[input_token].unsqueeze(0)\n",
    "    \n",
    "\n",
    "for _ in range(5):\n",
    "    with torch.no_grad():\n",
    "        output, (hidden, cell) = model.decoder(input_token, hidden, cell) # teacher_forcing_ratio=0.0\n",
    "        \n",
    "    # Obtener el token con la probabilidad más alta\n",
    "    best_guess = output.argmax(2).squeeze(0)\n",
    "    outputs.append(best_guess.item())\n",
    "        \n",
    "    # Si el token es <eos>, terminar la traducción\n",
    "    if best_guess == translation.vocab_es.stoi['<eos>']:\n",
    "        break\n",
    "        \n",
    "    # Utilizar la palabra predicha como la siguiente entrada al decoder\n",
    "    input_token = translation.vocab_es.vectors[best_guess].unsqueeze(0)\n",
    "        \n",
    "# Convertir los índices de salida a palabras\n",
    "translated_sentence = [translation.vocab_es.itos[idx] for idx in outputs]\n",
    "    \n",
    "result = ' '.join(translated_sentence)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
